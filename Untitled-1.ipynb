{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AE.csv\n",
      "Processed AF.csv\n",
      "Processed AL.csv\n",
      "Processed AO.csv\n",
      "Processed AR.csv\n",
      "Processed AT.csv\n",
      "Processed AZ.csv\n",
      "Processed BD.csv\n",
      "Processed BE.csv\n",
      "Processed BF.csv\n",
      "Processed BG.csv\n",
      "Processed BH.csv\n",
      "Processed BI.csv\n",
      "Processed BN.csv\n",
      "Processed BO.csv\n",
      "Processed BR.csv\n",
      "Processed BW.csv\n",
      "Processed CA.csv\n",
      "Processed CH.csv\n",
      "Processed CL.csv\n",
      "Processed CM.csv\n",
      "Processed CN.csv\n",
      "Processed CO.csv\n",
      "Processed CR.csv\n",
      "Processed CY.csv\n",
      "Processed CZ.csv\n",
      "Processed DE.csv\n",
      "Processed DJ.csv\n",
      "Processed DK.csv\n",
      "Processed DZ.csv\n",
      "Processed EC.csv\n",
      "Processed EE.csv\n",
      "Processed EG.csv\n",
      "Processed ES.csv\n",
      "Processed ET.csv\n",
      "Processed FI.csv\n",
      "Processed FJ.csv\n",
      "Processed FR.csv\n",
      "Processed GB.csv\n",
      "Processed GE.csv\n",
      "Processed GH.csv\n",
      "Processed GR.csv\n",
      "Processed GT.csv\n",
      "Processed HK.csv\n",
      "Processed HN.csv\n",
      "Processed HR.csv\n",
      "Processed HT.csv\n",
      "Processed HU.csv\n",
      "Processed ID.csv\n",
      "Processed IE.csv\n",
      "Processed IL.csv\n",
      "Processed IN.csv\n",
      "Processed IQ.csv\n",
      "Processed IR.csv\n",
      "Processed IS.csv\n",
      "Processed IT.csv\n",
      "Processed JM.csv\n",
      "Processed JO.csv\n",
      "Processed JP.csv\n",
      "Processed KH.csv\n",
      "Processed KR.csv\n",
      "Processed KW.csv\n",
      "Processed KZ.csv\n",
      "Processed LB.csv\n",
      "Processed LT.csv\n",
      "Processed LU.csv\n",
      "Processed LY.csv\n",
      "Processed MA.csv\n",
      "Processed MD.csv\n",
      "Processed MO.csv\n",
      "Processed MT.csv\n",
      "Processed MU.csv\n",
      "Processed MV.csv\n",
      "Processed MX.csv\n",
      "Processed MY.csv\n",
      "Processed NA.csv\n",
      "Processed NG.csv\n",
      "Processed NL.csv\n",
      "Processed NO.csv\n",
      "Processed OM.csv\n",
      "Processed PA.csv\n",
      "Processed PE.csv\n",
      "Processed PH.csv\n",
      "Processed PL.csv\n",
      "Processed PR.csv\n",
      "Processed PS.csv\n",
      "Processed PT.csv\n",
      "Processed QA.csv\n",
      "Processed RS.csv\n",
      "Processed RU.csv\n",
      "Processed SA.csv\n",
      "Processed SD.csv\n",
      "Processed SE.csv\n",
      "Processed SG.csv\n",
      "Processed SI.csv\n",
      "Processed SV.csv\n",
      "Processed SY.csv\n",
      "Processed TM.csv\n",
      "Processed TN.csv\n",
      "Processed TR.csv\n",
      "Processed TW.csv\n",
      "Processed US.csv\n",
      "Processed UY.csv\n",
      "Processed YE.csv\n",
      "Processed ZA.csv\n",
      "All files processed!\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "\n",
    "# data_folder = \"data\"\n",
    "# output_folder = \"processed_data\"\n",
    "\n",
    "# chunk_size = 50000  # Adjust based on your available memory\n",
    "\n",
    "# # Ensure the output directory exists\n",
    "# if not os.path.exists(output_folder):\n",
    "#     os.makedirs(output_folder)\n",
    "\n",
    "# # Iterate through every file in the data folder\n",
    "# for filename in os.listdir(data_folder):\n",
    "#     if filename.endswith(\".csv\"):\n",
    "#         input_filepath = os.path.join(data_folder, filename)\n",
    "#         output_filepath = os.path.join(output_folder, filename)\n",
    "        \n",
    "#         # Read the file in chunks\n",
    "#         chunks = pd.read_csv(input_filepath, chunksize=chunk_size, header=None)\n",
    "\n",
    "#         with open(output_filepath, 'w', newline='') as file:\n",
    "#             for chunk in chunks:\n",
    "#                 # Filter out rows where the third column is 'M' or 'F'\n",
    "#                 filtered_chunk = chunk[~chunk[2].isin(['M', 'F'])]\n",
    "                \n",
    "#                 # If it's the first chunk, write header, else append to file\n",
    "#                 if chunk.index[0] == 0:\n",
    "#                     filtered_chunk.to_csv(file, index=False, header=True)\n",
    "#                 else:\n",
    "#                     filtered_chunk.to_csv(file, mode='a', index=False, header=False)\n",
    "\n",
    "#         print(f\"Processed {filename}\")\n",
    "\n",
    "# print(\"All files processed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "data_folder = \"processed_data\"\n",
    "\n",
    "# 1. Combine and Label Your Data\n",
    "names = []\n",
    "labels = []\n",
    "\n",
    "# Read Turkish names\n",
    "turkish_df = pd.read_csv(os.path.join(data_folder, 'TR.csv'), header=None)\n",
    "names.extend(turkish_df[0].tolist())\n",
    "labels.extend([1] * turkish_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read names from other countries\n",
    "for filename in os.listdir(data_folder):\n",
    "    if filename.endswith(\".csv\") and filename != \"TR.csv\":\n",
    "        temp_df = pd.read_csv(os.path.join(data_folder, filename), header=None)\n",
    "        names.extend(temp_df[0].tolist())\n",
    "        labels.extend([0] * temp_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_names = [name for name in names if pd.notna(name)]\n",
    "cleaned_labels = [label for i, label in enumerate(labels) if pd.notna(names[i])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(2, 4))\n",
    "X = vectorizer.fit_transform(cleaned_names)\n",
    "y = cleaned_labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import save_npz\n",
    "save_npz('X_train.npz', X_train)\n",
    "save_npz('X_test.npz', X_test)\n",
    "import numpy as np\n",
    "np.save('y_train.npy', y_train)\n",
    "np.save('y_test.npy', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Users\\hcang\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Model Training\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the classifier\n",
    "with open('logistic_regression_model.pkl', 'wb') as f:\n",
    "    pickle.dump(clf, f)\n",
    "\n",
    "# Save the vectorizer\n",
    "with open('tfidf_vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(vectorizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9527\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Not Turkish       0.96      0.98      0.97  14594107\n",
      "     Turkish       0.93      0.84      0.88   3890067\n",
      "\n",
      "    accuracy                           0.95  18484174\n",
      "   macro avg       0.94      0.91      0.93  18484174\n",
      "weighted avg       0.95      0.95      0.95  18484174\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# 1. Predict on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# 2. Compare predicted labels with actual labels\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# 3. Additional evaluation metrics\n",
    "report = classification_report(y_test, y_pred, target_names=[\"Not Turkish\", \"Turkish\"])\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The name 'Mehmet' is predicted to be Turkish.\n"
     ]
    }
   ],
   "source": [
    "# Sample name to test\n",
    "name_to_test = \"Mehmet\"\n",
    "\n",
    "# 1. Transform the name\n",
    "name_vectorized = vectorizer.transform([name_to_test])\n",
    "\n",
    "# 2. Predict using the trained classifier\n",
    "prediction = clf.predict(name_vectorized)\n",
    "\n",
    "# Display the result\n",
    "if prediction[0] == 1:\n",
    "    print(f\"The name '{name_to_test}' is predicted to be Turkish.\")\n",
    "else:\n",
    "    print(f\"The name '{name_to_test}' is predicted to be NOT Turkish.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['David', 'Elizabeth', 'Emily', 'Grace', 'James', 'Michael', 'Olivia', 'Robert', 'Sarah', 'William']\n",
      "The name 'David' is predicted to be NOT Turkish.\n",
      "The name 'Elizabeth' is predicted to be NOT Turkish.\n",
      "The name 'Emily' is predicted to be NOT Turkish.\n",
      "The name 'Grace' is predicted to be NOT Turkish.\n",
      "The name 'James' is predicted to be NOT Turkish.\n",
      "The name 'Michael' is predicted to be NOT Turkish.\n",
      "The name 'Olivia' is predicted to be NOT Turkish.\n",
      "The name 'Robert' is predicted to be NOT Turkish.\n",
      "The name 'Sarah' is predicted to be NOT Turkish.\n",
      "The name 'William' is predicted to be NOT Turkish.\n"
     ]
    }
   ],
   "source": [
    "names = [\"Emily\", \"Michael\", \"Sarah\", \"James\", \"Olivia\", \"William\", \"Grace\", \"Robert\", \"Elizabeth\", \"David\"]\n",
    "\n",
    "names.sort()\n",
    "\n",
    "print(names)\n",
    "\n",
    "names_vectorized = vectorizer.transform(names)\n",
    "\n",
    "predictions = clf.predict(names_vectorized)\n",
    "\n",
    "for name, prediction in zip(names, predictions):\n",
    "    if prediction == 1:\n",
    "        print(f\"The name '{name}' is predicted to be Turkish.\")\n",
    "    else:\n",
    "        print(f\"The name '{name}' is predicted to be NOT Turkish.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ahmet', 'Ayşe', 'Elif', 'Emre', 'Fatma', 'Mehmet', 'Murat', 'Osman', 'Yasemin', 'Zeynep']\n",
      "The name 'Ahmet' is predicted to be Turkish.\n",
      "The name 'Ayşe' is predicted to be Turkish.\n",
      "The name 'Elif' is predicted to be Turkish.\n",
      "The name 'Emre' is predicted to be Turkish.\n",
      "The name 'Fatma' is predicted to be Turkish.\n",
      "The name 'Mehmet' is predicted to be Turkish.\n",
      "The name 'Murat' is predicted to be Turkish.\n",
      "The name 'Osman' is predicted to be Turkish.\n",
      "The name 'Yasemin' is predicted to be Turkish.\n",
      "The name 'Zeynep' is predicted to be Turkish.\n"
     ]
    }
   ],
   "source": [
    "turkish_names = [\"Ahmet\", \"Fatma\", \"Emre\", \"Ayşe\", \"Mehmet\", \"Zeynep\", \"Osman\", \"Elif\", \"Murat\", \"Yasemin\"]\n",
    "\n",
    "turkish_names.sort()\n",
    "\n",
    "print(turkish_names)\n",
    "\n",
    "turkish_names_vectorized = vectorizer.transform(turkish_names)\n",
    "\n",
    "turkish_predictions = clf.predict(turkish_names_vectorized)\n",
    "\n",
    "for name, prediction in zip(turkish_names, turkish_predictions):\n",
    "    if prediction == 1:\n",
    "        print(f\"The name '{name}' is predicted to be Turkish.\")\n",
    "    else:\n",
    "        print(f\"The name '{name}' is predicted to be NOT Turkish.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The name 'John Hakan Can Gunerli' is predicted to be Turkish.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "zeyno_name = \"John Gunerli\"\n",
    "# load vectorizer \n",
    "\n",
    "with open('tfidf_vectorizer.pkl', 'rb') as f:\n",
    "    vectorizer = pickle.load(f)\n",
    "    \n",
    "# load classifier\n",
    "\n",
    "with open('logistic_regression_model.pkl', 'rb') as f:\n",
    "    clf = pickle.load(f)\n",
    "\n",
    "\n",
    "zeyno_name_vectorized = vectorizer.transform([zeyno_name])\n",
    "\n",
    "zeyno_prediction = clf.predict(zeyno_name_vectorized)\n",
    "\n",
    "if zeyno_prediction[0] == 1:\n",
    "    print(f\"The name '{zeyno_name}' is predicted to be Turkish.\")\n",
    "else:\n",
    "    print(f\"The name '{zeyno_name}' is predicted to be NOT Turkish.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
